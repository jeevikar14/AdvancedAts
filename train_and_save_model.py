import pandas as pd
import numpy as np
import joblib
import os
import re
from sentence_transformers import SentenceTransformer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity

def extract_additional_features(resume_text, jd_text):
    """Simplified meaningful features for better generalization"""
    features = []
    
    # Text length ratio
    features.append(len(resume_text.split()) / (len(jd_text.split()) + 1))
    
    # Keyword overlap
    resume_words = set(resume_text.lower().split())
    jd_words = set(jd_text.lower().split())
    overlap = len(resume_words.intersection(jd_words))
    features.append(overlap / (len(jd_words) + 1))
    
    # Action verbs count
    action_verbs = ['managed', 'developed', 'created', 'implemented', 'led', 'designed', 
                    'built', 'improved', 'increased', 'reduced', 'achieved']
    features.append(sum(1 for verb in action_verbs if verb in resume_text.lower()))
    
    # Technical skills
    tech_skills = ['python', 'java', 'sql', 'javascript', 'machine learning', 'aws', 
                   'docker', 'kubernetes', 'react', 'angular', 'django', 'flask']
    features.append(sum(1 for skill in tech_skills if skill in resume_text.lower()))
    
    return np.array(features)

def train_and_save_model():
    print("ğŸš€ Starting Optimized ATS Model Training...")
    
    DATASET_PATH = 'cleaned_dataset.csv'
    MODEL_DIR = 'models'
    MODEL_PATH = os.path.join(MODEL_DIR, 'optimized_ats_model.pkl')
    os.makedirs(MODEL_DIR, exist_ok=True)
    
    if not os.path.exists(DATASET_PATH):
        print(f"âŒ Dataset not found at {DATASET_PATH}")
        return
    
    # Load dataset
    df = pd.read_csv(DATASET_PATH).dropna(subset=['jd_text', 'resume_text', 'score'])
    X_jd = df['jd_text'].values
    X_res = df['resume_text'].values
    y = df['score'].values
    
    # Load transformer
    print("ğŸ§  Loading SentenceTransformer...")
    model = SentenceTransformer('all-MiniLM-L6-v2')
    
    def get_embeddings_batch(texts, batch_size=32):
        embeddings = []
        for i in range(0, len(texts), batch_size):
            embeddings.extend(model.encode(texts[i:i+batch_size], show_progress_bar=False))
        return np.array(embeddings)
    
    print("âš™ï¸ Generating embeddings...")
    jd_embs = get_embeddings_batch(X_jd)
    res_embs = get_embeddings_batch(X_res)
    
    # PCA for dimensionality reduction (75 dims each)
    print("ğŸ”§ Reducing embedding dimensions with PCA...")
    pca_jd = PCA(n_components=75, random_state=42)
    pca_res = PCA(n_components=75, random_state=42)
    jd_embs_reduced = pca_jd.fit_transform(jd_embs)
    res_embs_reduced = pca_res.fit_transform(res_embs)
    
    # Cosine similarity
    cosine_sims = cosine_similarity(jd_embs, res_embs).diagonal().reshape(-1, 1)
    
    # Handcrafted features
    print("ğŸ”§ Extracting handcrafted features...")
    additional_features = np.array([extract_additional_features(X_res[i], X_jd[i]) 
                                    for i in range(len(X_res))])
    
    # Combine all features
    X_combined = np.hstack([jd_embs_reduced, res_embs_reduced, cosine_sims, additional_features])
    
    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(
        X_combined, y, test_size=0.2, random_state=42
    )
    
    # Scaling
    scaler = RobustScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Gradient Boosting with tuned parameters
    print("ğŸ‹ï¸ Training GradientBoosting...")
    gbr = GradientBoostingRegressor(
        n_estimators=200,
        max_depth=6,
        learning_rate=0.08,
        random_state=42
    )
    gbr.fit(X_train_scaled, y_train)
    
    # Evaluation
    y_train_pred = gbr.predict(X_train_scaled)
    y_test_pred = gbr.predict(X_test_scaled)
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    test_mae = mean_absolute_error(y_test, y_test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
    
    print(f"\nğŸ“Š Metrics:")
    print(f"   Train RÂ²: {train_r2:.4f}")
    print(f"   Test RÂ²: {test_r2:.4f}")
    print(f"   MAE: {test_mae:.2f}")
    print(f"   RMSE: {test_rmse:.2f}")
    
    # Save model
    model_data = {
        'model': gbr,
        'scaler': scaler,
        'pca_jd': pca_jd,
        'pca_res': pca_res,
        'feature_names': (
            ['jd_emb']*75 + ['res_emb']*75 + ['cosine_sim'] + 
            ['len_ratio','keyword_overlap','action_verbs','tech_skills']
        ),
        'metrics': {'train_r2': train_r2, 'test_r2': test_r2, 'mae': test_mae, 'rmse': test_rmse}
    }
    
    with open(MODEL_PATH, 'wb') as f:
        joblib.dump(model_data, f)
    
    print(f"âœ… Model saved to {MODEL_PATH}")
    print("ğŸ¯ Done! Test RÂ² should now reliably exceed 0.5.")

if __name__ == "__main__":
    train_and_save_model()
